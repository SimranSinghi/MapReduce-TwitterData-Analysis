The purpose of this project is to develop a simple Map-Reduce program on Hadoop that analyzes data from Twitter.
How to develop your project on your laptop
You can use your laptop to develop your program and then test it and run it on Comet. This step is optional but highly recommended because it will save you a lot of time. Note that testing and running your program on Comet is required.

If you have Mac OS or Linux, make sure you have Java and Maven installed. On Mac, you can install Maven using Homebrew: brew install maven. On Ubuntu Linux, use: apt install maven.

On Windows 10, you need to install Windows Subsystem for Linux (WSL 2) (Links to an external site.) and then Ubuntu 20.04 LTS. It's OK if you have WSL 1 or an older Ubuntu. Then, do on linux:

sudo apt update
sudo apt upgrade
sudo apt install openjdk-8-jdk maven
To install Hadoop and the project, cut&paste and execute:

cd
wget https://archive.apache.org/dist/hadoop/common/hadoop-2.6.5/hadoop-2.6.5.tar.gz
tar xfz hadoop-2.6.5.tar.gz
wget http://lambda.uta.edu/cse6331/project1.tgz
tar xfz project1.tgz
You should also set your JAVA_HOME to point to your java installation. For example, on Windows 10 do:

export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
To test Map-Reduce, go to project1/examples/src/main/java and look at the two Map-Reduce examples Simple.java and Join.java. You can compile both Java files using:

mvn install
and you can run Simple in standalone mode using:

~/hadoop-2.6.5/bin/hadoop jar target/*.jar Simple simple.txt output-simple
The file output-simple/part-r-00000 will contain the results.

To compile and run project1:

cd project1
mvn install
rm -rf temp output
~/hadoop-2.6.5/bin/hadoop jar target/*.jar Twitter small-twitter.csv temp output
The file output/part-r-00000 must contain the same results as in file small-solution.txt. (The file temp/part-r-00000 contains the temporary results from the first map-reduce job). After your project works correctly on your laptop (it produces the same results as the solution), copy it to Comet:

cd
rm project1.tgz
tar cfz project1.tgz project1
scp project1.tgz xyz1234@comet.sdsc.edu:
where xyz1234 is your Comet username.

Setting up your Project on Comet
This step is required. If you'd like, you can develop this project completely on Comet. Follow the directions on How to login to Comet at SDSC Comet. Please email the GTA if you need further help.

Edit the file .bashrc (note: it starts with a dot) using a text editor, such as nano .bashrc, and add the following 2 lines at the end (cut-and-paste):

export JAVA_HOME=/lib/jvm/java
alias run='srun --pty -A uot143 --partition=shared --nodes=1 --ntasks-per-node=1 --mem=5G -t 00:05:00 --wait=0 --export=ALL'
export project=/oasis/projects/nsf/uot143/fegaras
logout and login again to apply the changes. If you have already developed project1 on your laptop, copy project1.tgz from your laptop to Comet. Otherwise, download project1 from the class web site:

wget http://lambda.uta.edu/cse6331/project1.tgz
Untar it:

tar xfz project1.tgz
rm project1.tgz
chmod -R g-wrx,o-wrx project1
Go to project1/examples and look at the two Map-Reduce examples src/main/java/Simple.java and src/main/java/Join.java. You can compile both Java files using:

run example.build
and you can run them in standalone mode using:

sbatch example.local.run
The file example.local.out will contain the trace log of the Map-Reduce evaluation while the files output-simple/part-r-00000 output-join/part-r-00000 will contain the results.

You can compile Twitter.java on Comet using:

run twitter.build
and you can run Twitter.java in standalone mode over a small dataset using:

sbatch twitter.local.run
The results generated by your program will be in the directory output. Your results must be the same as in the file small-solution.txt.

You should develop and run your programs in standalone mode until you get the correct result. After you make sure that your program runs correctly in standalone mode, you run it in distributed mode using:

sbatch twitter.distr.run
This will process the data on the large dataset large-twitter.csv (located in /oasis/projects/nsf/uot143/fegaras/large-twitter.csv) and will write the result in the directory output-distr. These results should be similar to the results in the file large-solution.txt. Note that running in distributed mode will use up at least 10 of your SUs. So do this a couple of times only, after you make sure that your program works correctly in standalone mode.

Project Description
In this project, you are asked to implement a simple graph algorithm that needs two Map-Reduce jobs. You will use real data from Twitter from 2010. The dataset represents the follower graph that contains links between tweeting users in the form: user_id, follower_id (where user_id is the id of a user and follower_id is the id of the follower). For example:

12,13
12,14
12,15
16,17
Here, users 13, 14 and 15 are followers of user 12, while user 17 is a follower of user 16. The complete dataset is available on Comet (file /oasis/projects/nsf/uot143/fegaras/large-twitter.csv) and contains 736,930 users and 36,743,448 links. A subset of this file (which contains the last 10,000 lines of the complete dataset) is available in small-twitter.csv inside project1.

First, for each twitter user, you count the number of users she follows. Then, you group the users by their number of the users they follow and for each group you count how many users belong to this group. That is, the result will have lines such as:

10 30
which says that there are 30 users who follow 10 users. To help you, I am giving you the pseudo code. The first Map-Reduce is:

map ( key, line ):
  read 2 integers from the line into the variables id and follower_id (delimiter is comma ",")
  emit( follower_id, id )

reduce ( follower_id, ids ):
  count = 0
  for n in ids
      count++
  emit( follower_id, count )
That is, the ids in the reducer is the sequence of all users followed by the user with follower_id.
The second Map-Reduce is:

map ( key, line ):
  read 2 integers from the line into the variables follower_id and count (delimiter is tab "\t")
  emit( count, 1 )

reduce ( count, values ):
  sum = 0
  for v in values
      sum += v
  emit( count, sum )
You should write the two Map-Reduce job in the Java file src/main/java/Twitter.java. An empty src/main/java/Twitter.java has been provided, as well as scripts to build and run this code on Comet. You should modify the Twitter.java only. In your Java main program, args[0] is the twitter file, args[1] is the intermediate directory (output of job1 and input of job2), and args[2] is the output directory. All the input and output file formats must be text formats. Do not use binary formats. See slide 13 in bigdata-l03.pdf to see how to chain two jobs together. It will be easier to do this project if you finish the first map-reduce first and check the results in the temporary directory temp and then finish the second map-reduce.

Optional: Use an IDE to develop your project
If you have a prior good experience with an IDE (IntelliJ IDEA or Eclipse), you may want to develop your program using an IDE and then test it and run it on Comet. Using an IDE is optional; you shouldn't do this if you haven't used an IDE before.

On IntelliJ IDEA, go to New→Project from Existing Sources, then choose your project1 directory, select Maven, and then Finish. To compile the project, go to Run→Edit Configurations, use + to Add New Configuration, select Maven, give it a name, use Working directory: your project1 directory, Command line: install, then Apply.

On Eclipse, you first need to install m2e (Links to an external site.) (Maven on Eclipse), if it's not already installed. Then go to Open File...→Import Project from File System, then choose your project1 directory. To compile your project, right click on the project name at the Package Explorer, select Run As, and then Maven install.

Documentation
The The Map-Reduce API (Links to an external site.). The API has two variations for most classes: org.apache.hadoop.mapreduce and org.apache.hadoop.mapred. You should only use the classes in the package org.apache.hadoop.mapreduce
The org.apache.hadoop.mapreduce package (Links to an external site.)
The Job class
